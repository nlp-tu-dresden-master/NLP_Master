{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Algorithm for keyword extraction\n",
    "\n",
    "Please run inside directory with Corpora Class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from models.Corpora import Corpora\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defintion copied from other notebook i found online ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is another way to convert textual data to a numeric form and is short for Term Frequency-Inverse Document Frequency. The vector value it yields is the product of these two terms; TF and IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at Term Frequency. We have already looked at term frequency above with count vectorizer, but this time, we need one more step to calculate the relative frequency. Let's say we have two documents in total as below.\n",
    "\n",
    "1. I love dogs\n",
    "2. I hate dogs and knitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative term frequency is calculated for each term within each document as below.\n",
    "\n",
    "$${TF(t,d)} = \\frac {number\\ of\\ times\\ term(t)\\ appears\\ in\\ document(d)}{total\\ number\\ of\\ terms\\ in\\ document(d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate inverse document frequency for 'I',\n",
    "\n",
    "$${IDF('I',D)} = \\log \\Big(\\frac {2}{2}\\Big) = {0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the values for TF and IDF, now we can calculate TFIDF as below.\n",
    "\n",
    "$${TFIDF(t,d,D)} = {TF(t,d)}\\cdot{IDF(t,D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the case of our example, TFIDF for term 'I' in both documents will be as below.\n",
    "\n",
    "$${TFIDF('I',d1,D)} = {TF('I',d1)}\\cdot{IDF('I',D)} = {0.33}\\times{0} = {0}$$\n",
    "\n",
    "$${TFIDF('I',d2,D)} = {TF('I',d2)}\\cdot{IDF('I',D)} = {0.2}\\times{0} = {0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the term 'I' appeared equally in both documents, and the TFIDF score is 0, which means the term is not really informative in differentiating documents. The rest is same as count vectorizer, TFIDF vectorizer will calculate these scores for terms in documents, and convert textual data into a numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(algorithms: list, corpora: Corpora) -> dict:\n",
    "    \"\"\"\n",
    "    This method uses the tf-idf algorithm to determine the most relevant words in Corpus\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Defining all helper functions for tf*idf algorithm\n",
    "    \"\"\"\n",
    "    def frequency(word: str, document: list) -> int:\n",
    "        return document.count(word)\n",
    "\n",
    "    def number_of_words(doc: str) -> int:\n",
    "        return len(doc)\n",
    "\n",
    "    def term_frequency(word: str, document: list) -> float:\n",
    "        return float(frequency(word, document) / number_of_words(document))\n",
    "\n",
    "    def number_of_docs_containing_word(word: str, documents: list) -> float:\n",
    "        count: int = 0\n",
    "        for document in documents:\n",
    "            if frequency(word, document) > 0:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def inverse_document_freq(word: str, documents: list) -> float:\n",
    "        # log(Total number of documents / number of docs with the term)\n",
    "        return math.log(len(documents) / number_of_docs_containing_word(word, documents))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    result_dict: dict = {}\n",
    "    vocabulary: list = []\n",
    "    # For function in class Topic_Engine\n",
    "    # documents: list = [self.corpora.token_corpora[i.lower()] for i in algorithms]  # Already tokenized with RegExp\n",
    "    documents = [corpora.token_corpora[i.lower()] for i in algorithms]\n",
    "    for i, tokens in enumerate(documents):\n",
    "        doc_id = \"{}\".format(algorithms[i].lower())\n",
    "        \n",
    "        # Double cleaning ugly but necessary because there are lemmatized words, lemmatized to \"the\"\n",
    "        cleaned_tokens: list = [lemmatizer.lemmatize(token.lower()) for token in tokens if token not in stopwords]\n",
    "        cleaned_tokens = [t for t in cleaned_tokens if t not in stopwords]\n",
    "\n",
    "        bigram_tokens = bigrams(cleaned_tokens)  # Returns list of tupels\n",
    "        bigram_tokens = [' '.join(token) for token in bigram_tokens]\n",
    "\n",
    "        trigram_tokens: list = trigrams(cleaned_tokens)  # Returns list of tupels\n",
    "        trigram_tokens: list = [' '.join(token) for token in trigram_tokens]\n",
    "\n",
    "        all_tokens: list = []\n",
    "        all_tokens.extend(cleaned_tokens)\n",
    "        all_tokens.extend(bigram_tokens)\n",
    "        all_tokens.extend(trigram_tokens)\n",
    "\n",
    "        vocabulary.append(all_tokens)\n",
    "\n",
    "        result_dict.update({doc_id: {}})\n",
    "        for i, token in enumerate(all_tokens):\n",
    "            result_dict[doc_id].update({token: {}})\n",
    "            term_freq: float = term_frequency(token, all_tokens)\n",
    "            result_dict[doc_id][token].update({'term_frequency': term_freq})\n",
    "\n",
    "    for doc in result_dict:\n",
    "        for token in result_dict[doc]:\n",
    "            # Calculating IDF\n",
    "            result_dict[doc][token].update({\"inverse_document_frequency\": inverse_document_freq(token, vocabulary)})\n",
    "            # Calculating TF-IDF\n",
    "            result_dict[doc][token].update(\n",
    "                {\"tf-idf\": result_dict[doc][token][\"term_frequency\"] * result_dict[doc][token][\"inverse_document_frequency\"]})\n",
    "    \n",
    "    #  TODO Can be included in upper for loop for less code and little bit faster execution\n",
    "    # Build new dict with only \"token -> tf-idf\"\n",
    "    words = {}\n",
    "    for doc in result_dict:\n",
    "        words.update({doc: {}})\n",
    "        for token in result_dict[doc]:\n",
    "            if token not in words[doc]:\n",
    "                words[doc].update({token: result_dict[doc][token]['tf-idf']})\n",
    "            else:\n",
    "                if result_dict[doc][token]['tf-idf'] > words[doc][token]:\n",
    "                    words[doc].update({token: result_dict[doc][token]['tf-idf']})\n",
    "    \n",
    "    # Print out results\n",
    "    for doc in words:\n",
    "        words[doc] = sorted(words[doc].items(), key=lambda entry: entry[1], reverse=True)\n",
    "        print(\"\\n\\n###### Results for algorithm: \"+doc+\" ######\")\n",
    "        for i, token_and_score in enumerate(words[doc]):\n",
    "            print(token_and_score)\n",
    "            if i == 14: \n",
    "                break\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Results for algorithm: clustering ######\n",
      "('cluster', 0.012990710211297503)\n",
      "('object', 0.01113489446682643)\n",
      "('group', 0.009743032658473125)\n",
      "('clustering', 0.007887216914002054)\n",
      "('similar', 0.00463953936117768)\n",
      "('similarity', 0.004175585425059912)\n",
      "('based', 0.00231976968058884)\n",
      "('customer', 0.0018558157444710716)\n",
      "('goal', 0.0018558157444710716)\n",
      "('within', 0.0018558157444710716)\n",
      "('another', 0.0018558157444710716)\n",
      "('different', 0.0018558157444710716)\n",
      "('distance', 0.0018558157444710716)\n",
      "('mechanism', 0.0018558157444710716)\n",
      "('object cluster', 0.0018558157444710716)\n",
      "\n",
      "\n",
      "###### Results for algorithm: classification ######\n",
      "('model', 0.0036101415654163816)\n",
      "('classify', 0.0036101415654163816)\n",
      "('patient', 0.0036101415654163816)\n",
      "('large', 0.002406761043610921)\n",
      "('ing', 0.002406761043610921)\n",
      "('recognition', 0.002406761043610921)\n",
      "('resident', 0.002406761043610921)\n",
      "('prediction', 0.002406761043610921)\n",
      "('medical', 0.002406761043610921)\n",
      "('wish', 0.002406761043610921)\n",
      "('category', 0.002406761043610921)\n",
      "('observation', 0.002406761043610921)\n",
      "('assigning', 0.002406761043610921)\n",
      "('given', 0.002406761043610921)\n",
      "('spam', 0.002406761043610921)\n"
     ]
    }
   ],
   "source": [
    "from models import Corpora\n",
    "from models import Topic_Engine\n",
    "\n",
    "corp = Corpora([\"Clustering\"], [\"01_data/01_Clustering_definitions\"])\n",
    "corp.build_all_corpora_for_new_algorithm_type(\"Classification\", \"01_data/02_Classification_definitions\")\n",
    "\n",
    "td_idf_result = tf_idf(algorithms=[\"clustering\", \"Classification\"], corpora=corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
