{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Algorithm for keyword extraction\n",
    "\n",
    "Please run inside directory with Corpora Class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from models.Corpora import Corpora\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "from nltk import bigrams, trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defintion copied from other notebook i found online ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is another way to convert textual data to a numeric form and is short for Term Frequency-Inverse Document Frequency. The vector value it yields is the product of these two terms; TF and IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at Term Frequency. We have already looked at term frequency above with count vectorizer, but this time, we need one more step to calculate the relative frequency. Let's say we have two documents in total as below.\n",
    "\n",
    "1. I love dogs\n",
    "2. I hate dogs and knitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative term frequency is calculated for each term within each document as below.\n",
    "\n",
    "$${TF(t,d)} = \\frac {number\\ of\\ times\\ term(t)\\ appears\\ in\\ document(d)}{total\\ number\\ of\\ terms\\ in\\ document(d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate inverse document frequency for 'I',\n",
    "\n",
    "$${IDF('I',D)} = \\log \\Big(\\frac {2}{2}\\Big) = {0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the values for TF and IDF, now we can calculate TFIDF as below.\n",
    "\n",
    "$${TFIDF(t,d,D)} = {TF(t,d)}\\cdot{IDF(t,D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the case of our example, TFIDF for term 'I' in both documents will be as below.\n",
    "\n",
    "$${TFIDF('I',d1,D)} = {TF('I',d1)}\\cdot{IDF('I',D)} = {0.33}\\times{0} = {0}$$\n",
    "\n",
    "$${TFIDF('I',d2,D)} = {TF('I',d2)}\\cdot{IDF('I',D)} = {0.2}\\times{0} = {0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the term 'I' appeared equally in both documents, and the TFIDF score is 0, which means the term is not really informative in differentiating documents. The rest is same as count vectorizer, TFIDF vectorizer will calculate these scores for terms in documents, and convert textual data into a numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(algorithms: list, corpora: Corpora) -> dict:\n",
    "    \"\"\"\n",
    "    This method uses the tf-idf algorithm to determine the most relevant words in Corpus\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Defining all helper functions for tf*idf algorithm\n",
    "    \"\"\"\n",
    "    def frequency(word: str, document: list) -> int:\n",
    "        return document.count(word)\n",
    "\n",
    "    def number_of_words(doc: str) -> int:\n",
    "        return len(doc)\n",
    "\n",
    "    def term_frequency(word: str, document: list) -> float:\n",
    "        return float(frequency(word, document) / number_of_words(document))\n",
    "\n",
    "    def number_of_docs_containing_word(word: str, documents: list) -> float:\n",
    "        count: int = 0\n",
    "        for document in documents:\n",
    "            if frequency(word, document) > 0:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def inverse_document_freq(word: str, documents: list) -> float:\n",
    "        # log(Total number of documents / number of docs with the term)\n",
    "        return math.log(len(documents) / number_of_docs_containing_word(word, documents))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    result_dict: dict = {}\n",
    "    vocabulary: list = []\n",
    "    # For function in class Topic_Engine\n",
    "    # documents: list = [self.corpora.token_corpora[i.lower()] for i in algorithms]  # Already tokenized with RegExp\n",
    "    documents = [corpora.token_corpora[i.lower()] for i in algorithms]\n",
    "    for i, tokens in enumerate(documents):\n",
    "        doc_id = \"{}\".format(algorithms[i].lower())\n",
    "        \n",
    "        # Double cleaning ugly but necessary because there are lemmatized words, lemmatized to \"the\"\n",
    "        cleaned_tokens: list = [lemmatizer.lemmatize(token.lower()) for token in tokens if token not in stopwords]\n",
    "        cleaned_tokens = [t for t in cleaned_tokens if t not in stopwords]\n",
    "\n",
    "        bigram_tokens = bigrams(cleaned_tokens)  # Returns list of tupels\n",
    "        bigram_tokens = [' '.join(token) for token in bigram_tokens]\n",
    "\n",
    "        trigram_tokens: list = trigrams(cleaned_tokens)  # Returns list of tupels\n",
    "        trigram_tokens: list = [' '.join(token) for token in trigram_tokens]\n",
    "\n",
    "        all_tokens: list = []\n",
    "        all_tokens.extend(cleaned_tokens)\n",
    "        all_tokens.extend(bigram_tokens)\n",
    "        all_tokens.extend(trigram_tokens)\n",
    "\n",
    "        vocabulary.append(all_tokens)\n",
    "\n",
    "        result_dict.update({doc_id: {}})\n",
    "        for i, token in enumerate(all_tokens):\n",
    "            result_dict[doc_id].update({token: {}})\n",
    "            term_freq: float = term_frequency(token, all_tokens)\n",
    "            result_dict[doc_id][token].update({'term_frequency': term_freq})\n",
    "\n",
    "    for doc in result_dict:\n",
    "        for token in result_dict[doc]:\n",
    "            # Calculating IDF\n",
    "            result_dict[doc][token].update({\"inverse_document_frequency\": inverse_document_freq(token, vocabulary)})\n",
    "            # Calculating TF-IDF\n",
    "            result_dict[doc][token].update(\n",
    "                {\"tf-idf\": result_dict[doc][token][\"term_frequency\"] * result_dict[doc][token][\"inverse_document_frequency\"]})\n",
    "    \n",
    "    #  TODO Can be included in upper for loop for less code and little bit faster execution\n",
    "    # Build new dict with only \"token -> tf-idf\"\n",
    "    words = {}\n",
    "    for doc in result_dict:\n",
    "        words.update({doc: {}})\n",
    "        for token in result_dict[doc]:\n",
    "            if token not in words[doc]:\n",
    "                words[doc].update({token: result_dict[doc][token]['tf-idf']})\n",
    "            else:\n",
    "                if result_dict[doc][token]['tf-idf'] > words[doc][token]:\n",
    "                    words[doc].update({token: result_dict[doc][token]['tf-idf']})\n",
    "    \n",
    "    # Print out results\n",
    "    for doc in words:\n",
    "        words[doc] = sorted(words[doc].items(), key=lambda entry: entry[1], reverse=True)\n",
    "        print(\"\\n\\n###### Results for algorithm: \"+doc+\" ######\")\n",
    "        for i, token_and_score in enumerate(words[doc]):\n",
    "            print(token_and_score)\n",
    "            if i == 14: \n",
    "                break\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base text for clustering:\n",
      "\n",
      " Clustering algorithms examine data to find groups of items that are similar. For example, an insurance company might group customers according to income, age, types of policy purchased or prior claims experience. In a fault diagnosis application, electrical faults might be grouped according to the values of certain key variables.\n",
      " Clustering is the process of making a group of abstract objects into classes of similar objects. Cluster analysis groups data objects based only on information found in the data that describes the objects and their relationships. The goal is that the objects within a group be similar (or related) to one another and different from (or unrelated to) the objects in other groups. The greater the similarity (or homogeneity) within a group and the greater the difference between groups, the better or more distinct the clustering. Clustering is the grouping of a particular set of objects based on their characteristics, aggregating them according to their similarities. Regarding to data mining, this metodology partitions the data implementing a specific join algorithm, most suitable for the desired information analysis. Clustering can be defined as the process of grouping a collection of N patterns into distinct segments or clusters based on a suitable notion of closeness or similarity among these patterns. Good clusters show high similarity within a group and low similarity between patterns belonging to two different groups. For applications such as customer or product segmentation, clustering is the primary goal. But more often it is a crucial intermediate step needed for further data analysis, a view underscored by the placement of the cluster utility in the \"exploration\" stage in Enterprise Miner, a leading data mining software from SAS institute. Clustering can be considered the most important unsupervised learning problem; so, as every other problem of this kind, it deals with finding a structure in a collection of unlabeled data.\n",
      "A loose definition of clustering could be “the process of organizing objects into groups whose members are similar in some way”.\n",
      "A cluster is therefore a collection of objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters. Clustering techniques consider data tuples as objects. They partition the objects into groups, or clusters, so that objects within a cluster are “similar” to one another and “dis- similar” to objects in other clusters. Similarity is commonly defined in terms of how “close” the objects are in space, based on a distance function. The “quality” of a cluster may be represented by its diameter, the maximum distance between any two objects in the cluster. Centroid distance is an alternative measure of cluster quality and is defined as the average distance of each cluster object from the cluster centroid (denoting the “average object,” or average point in space for the cluster). Figure 3.3 showed a 2-D plot of customer data with respect to customer locations in a city. Three data clusters are visible.\n",
      "In data reduction, the cluster representations of the data are used to replace the actual data. The effectiveness of this technique depends on the data’s nature. It is much more effective for data that can be organized into distinct clusters than for smeared data. Data clustering is a method in which we make cluster of objects that are somehow similar in characteristics. The criterion for checking the similarity is implementation dependent. Clustering techniques apply when there is no class to be predicted but rather when the instances are to be decided into natural groups. These clusters presumably reflect some mechanism at work in the domain from which instances are drawn, a mechanism that causes some instances to bear a stronger resemblance to each other than they do to the remaining instances. Clustering naturally required different techniques to the classification and association learning methods that we have considered so far. As we saw in Section 3.6, there are different ways in which the result of clustering can be expressed. The groups that are identified may be exclusive: any instance belongs in only one group. Or they may be overlapping: an instance may fall into several groups. Ot they may be probabilistic: an instance belongs to each group with a certain probability. Or they may be hierarchical: a rough division of instances into groups at the top level and each group refined further - perhaps all the way down to individual instances. Really, the choice among these possibilities should be dictated by the nature of the mechanisms that are thought to underlie the particular clustering phenomenon. However, because these mechanisms are rarely known - the very existence of clusters is, after all, something that we’re trying to discover - and for pragmatic reasons too, the choice is usually dictated by the clustering tools that are available. Cluster Analysis technique as a field grew very quickly with the goal of grouping data objects, based on information found in data and describing the relationships inside the data. The purpose is to separate the objects into groups, with the objects related (similar) together and unrelated with another group of objects. It is being applied in variety of science disciplines and has been studied in myriad of expert research communities such as machine learning, statistic, optimization and computational geometry. Clustering analysis finds clusters of data objects that are similar in some sense to one another. The members of a cluster are more like each other than they are like members of other clusters. The goal of clustering analysis is to find high-quality clusters such that the inter-cluster similarity is low and the intra-cluster similarity is high.\n",
      "\n",
      "\n",
      "\n",
      "Using tf-idf:\n",
      "\n",
      "\n",
      "###### Results for algorithm: clustering ######\n",
      "('cluster', 0.012990710211297503)\n",
      "('object', 0.01113489446682643)\n",
      "('group', 0.009743032658473125)\n",
      "('clustering', 0.007887216914002054)\n",
      "('similar', 0.00463953936117768)\n",
      "('similarity', 0.004175585425059912)\n",
      "('based', 0.00231976968058884)\n",
      "('customer', 0.0018558157444710716)\n",
      "('goal', 0.0018558157444710716)\n",
      "('within', 0.0018558157444710716)\n",
      "('another', 0.0018558157444710716)\n",
      "('different', 0.0018558157444710716)\n",
      "('distance', 0.0018558157444710716)\n",
      "('mechanism', 0.0018558157444710716)\n",
      "('object cluster', 0.0018558157444710716)\n",
      "\n",
      "\n",
      "###### Results for algorithm: classification ######\n",
      "('model', 0.0036101415654163816)\n",
      "('classify', 0.0036101415654163816)\n",
      "('patient', 0.0036101415654163816)\n",
      "('large', 0.002406761043610921)\n",
      "('ing', 0.002406761043610921)\n",
      "('recognition', 0.002406761043610921)\n",
      "('resident', 0.002406761043610921)\n",
      "('prediction', 0.002406761043610921)\n",
      "('medical', 0.002406761043610921)\n",
      "('wish', 0.002406761043610921)\n",
      "('category', 0.002406761043610921)\n",
      "('observation', 0.002406761043610921)\n",
      "('assigning', 0.002406761043610921)\n",
      "('given', 0.002406761043610921)\n",
      "('spam', 0.002406761043610921)\n"
     ]
    }
   ],
   "source": [
    "from models import Corpora\n",
    "from models import Topic_Engine\n",
    "\n",
    "corp = Corpora([\"Clustering\"], [\"01_data/01_Clustering_definitions\"])\n",
    "corp.build_all_corpora_for_new_algorithm_type(\"Classification\", \"01_data/02_Classification_definitions\")\n",
    "\n",
    "print(\"Base text for clustering:\\n\")\n",
    "print(corp.raw_corpora[\"clustering\"])\n",
    "print(\"\\n\\n\")\n",
    "print(\"Using tf-idf:\")\n",
    "td_idf_result = tf_idf(algorithms=[\"clustering\", \"Classification\"], corpora=corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
