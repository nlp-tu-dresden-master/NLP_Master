{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Textrank Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytextrank import *\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from models.Corpora import Corpora\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Raw Corpus Data to JSON\n",
    "Why JSON? The used Plugin for TextRank relies on JSON. It's possible to change that but it goes hand in hand with more work. For the first draft we just parse the text into some JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpora(paths=[\"01_data/01_Clustering_definitions\"], names=[\"Clustering\"])\n",
    "raw_corpus = {\"id\": 777, \"text\": corpus.raw_corpora[\"clustering\"]}\n",
    "\n",
    "with open(\"data.json\", \"w\") as f_json:\n",
    "    json.dump(raw_corpus, f_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input files and output files for the different stages\n",
    "***Stage 0 ***: Input JSON\n",
    "\n",
    "***Stage 1***: perform statistical parsing/tagging on a document in JSON format\n",
    "\n",
    "***Stage 2***: collect and normalize the key phrases from a parsed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stage0 = \"data.json\"\n",
    "path_stage1 = \"out_1.json\"  # Cleaning Data and such stuff\n",
    "path_stage2 = \"out_2.json\"  # Text Rank Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Warning: no model found for 'en'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n    Only loading the 'en' tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sentence boundary detection requires the dependency parse, which requires data to be installed. If you haven't done so, run: \npython -m spacy download en\nto install the data",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f13b5f0fc6eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_stage1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mgraf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparse_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_stage0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_asdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\pytextrank\\pytextrank.py\u001b[0m in \u001b[0;36mparse_doc\u001b[1;34m(json_iter)\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"graf_text:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraf_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0mgrafs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_base_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_graf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraf_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m             \u001b[0mbase_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_base_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\pytextrank\\pytextrank.py\u001b[0m in \u001b[0;36mparse_graf\u001b[1;34m(doc_id, graf_text, base_idx, spacy_nlp)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy_nlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraf_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[0mgraf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mdigest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msha1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tfdeeplearning\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36m__get__ (spacy/tokens/doc.cpp:9664)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: sentence boundary detection requires the dependency parse, which requires data to be installed. If you haven't done so, run: \npython -m spacy download en\nto install the data"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with open(path_stage1, 'w') as f:\n",
    "    for graf in parse_doc(json_iter(path_stage0)):\n",
    "        f.write(\"%s\\n\" % pretty_print(graf._asdict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
